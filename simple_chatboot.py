from dotenv import load_dotenv
from typing import Annotated
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain.chat_models import init_chat_model
from typing_extensions import TypedDict

load_dotenv()

llm = init_chat_model("groq:llama3-70b-8192")


# Define the message schema
class State(TypedDict):
    """State for the graph, containing the messages.

    Attributes:
        messages (list): A list of messages in the conversation.
    """

    messages: Annotated[list, add_messages]


# Initialize the state graph
# This graph will manage the state of the conversation
# and the flow of messages.
# It starts with the initial state and ends with the final state.
# The graph will be built using the StateGraph class.
graph_builder = StateGraph(State)


def chatboot(state: State) -> dict:
    """Handles the chat interaction.

    This function takes the current state, invokes the LLM with the
    messages in the state, and returns the updated state with the new message.

    Args:
        state (State):
            The current state of the conversation, containing messages.

    Returns:
        dict:
            A dictionary containing the updated messages after invoking the LLM.
    """
    return {"messages": [llm.invoke(state["messages"])]}


# Define the nodes and edges of the graph
# The graph consists of a single node that handles the chat interaction.
# The node is connected to the start and end of the graph.
graph_builder.add_edge(START, "chatboot")
graph_builder.add_node("chatboot", chatboot)
graph_builder.add_edge("chatboot", END)


# Compile the graph to finalize its structure
# This step prepares the graph for execution by resolving all nodes and edges.
# The compiled graph can then be used to invoke the chat interaction.
graph = graph_builder.compile()


if __name__ == "__main__":
    # Print the ASCII representation of the graph for debugging
    # This will show the structure of the graph in a text format.
    # It helps visualize the flow of messages and the nodes in the graph.
    print(graph.get_graph().draw_ascii())

    # Main loop to interact with the chatboot
    # This loop will continuously prompt the user for input,
    # invoke the graph with the user's message, and print the response.
    user_input = input("Add a message: ")

    # Invoke the graph with the user's input
    # The user's message is added to the state, and the graph processes it.
    # The response from the LLM is printed to the console.
    state = graph.invoke({
        "messages": [{"role": "user", "content": user_input}]
    })

    # Print the last message from the state
    # This is the response generated by the LLM based on the user's input.
    print(state["messages"][-1].content)
